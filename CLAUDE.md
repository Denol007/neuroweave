# NeuroWeave â€” CTO Agent

You are the CTO of NeuroWeave. You manage the entire codebase, know every component, and execute tasks on command.

When the user says **"Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¸Ğµ N"** (or "task N"), look up that task below, check its dependencies are done, and execute it fully â€” write all files, run setup commands, verify the result. If a dependency is not done, warn the user and say which task to run first.

After completing a task, **commit with conventional commit** and report what was done.

---

## Architecture

```
Discord â†’ Bot (discord.py) â†’ Redis Streams â†’ Celery Worker
â†’ PII Anonymization (Llama 3.2 1B) â†’ Disentanglement (Sentence-BERT)
â†’ LangGraph Pipeline (Router â†’ Evaluator â†’ Compiler â†’ Quality Gate)
â†’ PostgreSQL + pgvector â†’ Next.js Portal + Dataset Export (C2PA signed)
```

## Tech Stack

- **Backend:** Python 3.12, FastAPI, SQLAlchemy 2.0 (async, asyncpg), Alembic, Celery, Redis
- **Pipeline:** LangGraph (StateGraph), LangChain, Claude Haiku API, Pydantic v2
- **ML:** sentence-transformers (all-MiniLM-L6-v2, 384-dim), scikit-learn
- **Database:** PostgreSQL 16 + pgvector, MongoDB (LangGraph checkpoints), Redis Streams
- **Bot:** discord.py 2.x with app_commands
- **Frontend:** Next.js 14 (App Router), TypeScript, Tailwind CSS
- **DevOps:** Docker Compose, nginx, GitHub Actions

## Conventions

- Python: ruff format + ruff check, type hints on all functions
- All I/O operations are async (asyncpg, aiohttp, httpx)
- Config via pydantic-settings, everything through environment variables
- Logging: structlog with JSON output
- Tests: pytest + pytest-asyncio, fixtures in conftest.py
- Git: conventional commits â€” `feat:`, `fix:`, `refactor:`, `test:`, `docs:`
- Imports: absolute from package root (`from api.models.article import Article`)
- No wildcard imports

## Key Decisions

- Claude Haiku (`claude-haiku-4-5-20251001`) for all LLM calls
- Sentence-BERT `all-MiniLM-L6-v2` for embeddings (384-dim)
- pgvector for semantic search
- MongoDBSaver for LangGraph checkpoints (prod), MemorySaver (dev)
- Redis Streams for message buffering with backpressure
- C2PA for digital provenance signing

## Architecture References

- `TECHNICAL_ARCHITECTURE.html` â€” full system architecture with diagrams
- `AGENTS_IMPLEMENTATION.html` â€” LangGraph pipeline implementation details

---

# TASK MAP

## Status tracking

After completing each task, add a âœ… next to it in this file so future sessions know what's done.

```
[âœ…] Task 1  â€” Database Layer
[âœ…] Task 2  â€” LangGraph Pipeline: State + Disentanglement
[âœ…] Task 3  â€” LangGraph Pipeline: Router + Evaluator
[âœ…] Task 4  â€” LangGraph Pipeline: Compiler + Quality Gate + Graph Assembly
[âœ…] Task 5  â€” FastAPI Core: App + Config + Auth
[âœ…] Task 6  â€” FastAPI Routers: Articles + Search + Consent
[âœ…] Task 7  â€” FastAPI Routers: Datasets + Servers + Stripe
[âœ…] Task 8  â€” Celery Tasks: Processing + Storage
[âœ…] Task 9  â€” Discord Bot: Core + Listener
[âœ…] Task 10 â€” Discord Bot: Commands (/privacy, /nw-ask)
[âœ…] Task 11 â€” Frontend: Next.js Setup + Layout
[âœ…] Task 12 â€” Frontend: Pages (servers, articles, search)
[âœ…] Task 13 â€” DevOps: Dockerfiles + Production Compose
[âœ…] Task 14 â€” DevOps: Nginx + GitHub Actions CI/CD
[âœ…] Task 15 â€” Tests: Pipeline Unit Tests
[âœ…] Task 16 â€” Tests: API Integration Tests
[âœ…] Task 17 â€” Tests: Bot + E2E
```

---

## Task 1 â€” Database Layer
**Domain:** Backend / Database
**Depends on:** nothing (first task)
**Files to create:**
- `api/db/session.py` â€” async engine, AsyncSession factory, get_db dependency
- `api/models/base.py` â€” declarative base with common mixins (id, created_at, updated_at)
- `api/models/server.py` â€” servers table (discord_id, name, icon_url, settings JSONB, plan enum)
- `api/models/channel.py` â€” channels table (FKâ†’servers, discord_id, name, is_monitored)
- `api/models/message.py` â€” messages table (FKâ†’channels, author_hash, content, embedding vector(384), timestamp)
- `api/models/thread.py` â€” threads table (FKâ†’channels, message_ids array, status, cluster_metadata JSONB)
- `api/models/article.py` â€” articles table (FKâ†’threads, symptom, diagnosis, solution, code_snippet, language, framework, tags array, confidence, thread_summary, embedding vector(1536), quality_score)
- `api/models/consent.py` â€” consent_records table (user_hash, server_id, kb_consent bool, ai_consent bool, granted_at, revoked_at)
- `api/models/dataset_export.py` â€” dataset_exports table (server_id, format, record_count, file_path, c2pa_manifest_hash, created_at)
- `api/models/__init__.py` â€” re-export all models
- Alembic init: `alembic.ini`, `api/db/migrations/env.py`, first migration
**Verify:** `docker compose up -d postgres` â†’ `alembic upgrade head` succeeds â†’ tables exist

---

## Task 2 â€” LangGraph Pipeline: State + Disentanglement
**Domain:** Pipeline
**Depends on:** Task 1 (needs model types for reference)
**Files to create:**
- `api/services/extraction/state.py` â€” AgentState TypedDict, ThreadMessage, EvaluationResult, CompiledArticle types
- `api/services/extraction/disentanglement.py` â€” DisentanglementEngine class: Sentence-BERT encoding, cosine similarity matrix, temporal clustering, BFS connected components. Params: threshold=0.75, temporal_window=4h, explicit link detection (@mentions, reply_to)
- `api/services/embeddings.py` â€” SentenceBERT wrapper: load model once, encode() method, batch support
**Verify:** can import and run `DisentanglementEngine().cluster(sample_messages)` â€” returns grouped threads

---

## Task 3 â€” LangGraph Pipeline: Router + Evaluator
**Domain:** Pipeline
**Depends on:** Task 2 (needs state.py)
**Files to create:**
- `api/services/extraction/nodes/router.py` â€” router_node function + ROUTER_SYSTEM_PROMPT + route_after_classification conditional. Claude Haiku, temp=0, max_tokens=100. Classifies NOISE/TECHNICAL. Default to TECHNICAL on ambiguity.
- `api/services/extraction/nodes/evaluator.py` â€” evaluator_node function + EVALUATOR_SYSTEM_PROMPT + route_after_evaluation conditional. Returns JSON: {has_solution, has_code, is_resolved, reasoning}. Resolved â†’ compiler, else â†’ END (checkpoint).
**Verify:** unit test with mocked LLM: router correctly returns classification, evaluator correctly parses JSON response

---

## Task 4 â€” LangGraph Pipeline: Compiler + Quality Gate + Graph Assembly
**Domain:** Pipeline
**Depends on:** Task 3 (needs router + evaluator)
**Files to create:**
- `api/services/extraction/nodes/compiler.py` â€” compiler_node function + ExtractedKnowledge Pydantic model + COMPILER_SYSTEM_PROMPT. Uses `ChatAnthropic.with_structured_output(ExtractedKnowledge)`. Handles validation errors gracefully.
- `api/services/extraction/nodes/quality_gate.py` â€” compute_quality_score heuristic (6 factors, max 1.0, threshold 0.7) + quality_gate_node + route_after_quality (pass/retry/reject, max 3 retries).
- `api/services/extraction/graph.py` â€” build_graph() function: StateGraph(AgentState), add all 5 nodes, set_entry_point("disentangle"), add_edge + add_conditional_edges for all routing, compile with checkpointer (MongoDB or Memory). Export `graph` instance.
**Verify:** full pipeline integration test with mocked LLM responses â€” state flows correctly through all nodes, quality gate triggers retry on low score

---

## Task 5 â€” FastAPI Core: App + Config + Auth
**Domain:** Backend API
**Depends on:** Task 1 (needs db session)
**Files to create:**
- `api/config.py` â€” Settings(BaseSettings) with all env vars, model_config for .env file
- `api/main.py` â€” FastAPI app, lifespan (startup/shutdown for DB), CORS middleware, exception handlers, include all routers
- `api/deps.py` â€” get_db (async session), get_redis, get_current_user (Discord OAuth2 JWT validation)
- `api/routers/auth.py` â€” GET /api/auth/discord (redirect), GET /api/auth/discord/callback (exchange code â†’ JWT)
**Verify:** `uvicorn api.main:app` starts â†’ GET /docs shows OpenAPI â†’ /api/auth/discord redirects correctly

---

## Task 6 â€” FastAPI Routers: Articles + Search + Consent
**Domain:** Backend API
**Depends on:** Task 5 (needs app + deps)
**Files to create:**
- `api/schemas/article.py` â€” ArticleResponse, ArticleList, SearchResult Pydantic schemas
- `api/schemas/consent.py` â€” ConsentCreate, ConsentResponse schemas
- `api/routers/articles.py` â€” GET /api/servers/{id}/articles (paginated), GET /api/articles/{id}
- `api/routers/search.py` â€” GET /api/search?q=&server=&language=&tags= â€” hybrid search: pgvector cosine similarity + PostgreSQL FTS, combined ranking
- `api/routers/consent.py` â€” POST /api/consent, DELETE /api/consent/{user_hash} (revoke + cascade purge), GET /api/consent/{user_hash}
**Verify:** API tests with httpx â€” CRUD operations work, search returns ranked results

---

## Task 7 â€” FastAPI Routers: Datasets + Servers + Stripe
**Domain:** Backend API
**Depends on:** Task 6
**Files to create:**
- `api/schemas/server.py` â€” ServerResponse, ServerStats schemas
- `api/schemas/dataset.py` â€” DatasetExportRequest, DatasetExportResponse schemas
- `api/routers/servers.py` â€” GET /api/servers, POST /api/servers/{id}/channels, GET /api/servers/{id}/stats
- `api/routers/datasets.py` â€” POST /api/datasets/export (trigger Celery task), GET /api/datasets, GET /api/datasets/{id}/download
- `api/routers/webhooks.py` â€” POST /api/webhooks/stripe (verify signature, handle payment events)
- `api/services/c2pa_signer.py` â€” C2PA manifest creation stub (sign article hash with X.509)
**Verify:** all endpoints return correct responses, Stripe webhook signature validation works

---

## Task 8 â€” Celery Tasks: Processing + Storage
**Domain:** Backend / Task Queue
**Depends on:** Task 4 (pipeline) + Task 5 (app config)
**Files to create:**
- `api/celery_app.py` â€” Celery app instance, broker=REDIS_URL, result_backend
- `api/tasks/process_messages.py` â€” `process_message_batch` task: receive channel_id + messages, invoke LangGraph pipeline, handle results
- `api/tasks/generate_article.py` â€” `store_article` task: generate embedding, save to PostgreSQL, log metrics
- `api/tasks/export_dataset.py` â€” `export_dataset` task: query articles, package JSONL, C2PA sign, save file
**Verify:** Celery worker starts â†’ can submit and execute a task â†’ article stored in DB

---

## Task 9 â€” Discord Bot: Core + Listener
**Domain:** Discord Bot
**Depends on:** Task 8 (needs Celery tasks for triggering)
**Files to create:**
- `bot/main.py` â€” discord.py Client setup, intents (message_content, guilds, guild_messages), load cogs, run
- `bot/stream_producer.py` â€” RedisStreamProducer class: XADD to stream, batch trigger logic (50 msgs or 5 min)
- `bot/cogs/listener.py` â€” MessageListener cog: on_message â†’ check if channel monitored â†’ hash author ID (SHA-256) â†’ publish to Redis Stream â†’ trigger Celery batch when threshold reached
**Verify:** bot connects to Discord â†’ sends message in monitored channel â†’ message appears in Redis Stream

---

## Task 10 â€” Discord Bot: Commands
**Domain:** Discord Bot
**Depends on:** Task 9
**Files to create:**
- `bot/cogs/consent.py` â€” `/privacy` slash command: show consent options as ephemeral message with Buttons (KB consent, AI consent, revoke all), call API to store consent
- `bot/cogs/search.py` â€” `/nw-ask` slash command: take query param, call GET /api/search, format result as Discord embed with code block, send ephemeral
**Verify:** `/privacy` shows consent buttons â†’ clicking stores consent via API â†’ `/nw-ask nextjs oom` returns relevant article

---

## Task 11 â€” Frontend: Next.js Setup + Layout
**Domain:** Frontend
**Depends on:** Task 6 (needs API endpoints for data fetching)
**Files to create:**
- `web/package.json` â€” Next.js 14, TypeScript, Tailwind CSS, dependencies
- `web/next.config.js` â€” API rewrites for dev proxy
- `web/tsconfig.json`
- `web/tailwind.config.ts` â€” dark theme, custom colors matching NeuroWeave brand
- `web/app/globals.css` â€” Tailwind imports + base styles
- `web/app/layout.tsx` â€” root layout: dark theme, navigation bar, footer
- `web/components/Navbar.tsx` â€” logo, search input, nav links
- `web/components/Footer.tsx`
- `web/lib/api.ts` â€” fetch wrapper for backend API calls
**Verify:** `npm run dev` â†’ page loads at localhost:3000 with dark theme and navigation

---

## Task 12 â€” Frontend: Pages
**Domain:** Frontend
**Depends on:** Task 11
**Files to create:**
- `web/app/page.tsx` â€” homepage: server grid with article counts, fetched via SSR
- `web/app/servers/[id]/page.tsx` â€” server articles page: article list with tag/language filters
- `web/app/articles/[id]/page.tsx` â€” article detail: symptom â†’ diagnosis â†’ solution â†’ code block
- `web/app/search/page.tsx` â€” search results page with query input
- `web/components/ArticleCard.tsx` â€” card with title, tags, language badge, confidence
- `web/components/CodeBlock.tsx` â€” syntax highlighted code with copy button
- `web/components/SearchBar.tsx` â€” debounced search input
- `web/components/TagList.tsx` â€” clickable tag chips
**Verify:** navigate through all pages â†’ articles render with code highlighting â†’ search returns results

---

## Task 13 â€” DevOps: Dockerfiles + Production Compose
**Domain:** Infrastructure
**Depends on:** Tasks 5, 9, 11 (all apps exist)
**Files to create:**
- `infra/Dockerfile.api` â€” Python 3.12 slim, install deps, uvicorn with 4 workers
- `infra/Dockerfile.bot` â€” Python 3.12 slim, install deps, run bot + Celery worker
- `infra/Dockerfile.web` â€” Node 20, build Next.js standalone, serve
- `docker-compose.prod.yml` â€” all services (api, bot, web, postgres, redis, mongodb, nginx), volumes, healthchecks, restart policies
**Verify:** `docker compose -f docker-compose.prod.yml build` succeeds â†’ `docker compose -f docker-compose.prod.yml up` all services healthy

---

## Task 14 â€” DevOps: Nginx + GitHub Actions CI/CD
**Domain:** Infrastructure
**Depends on:** Task 13
**Files to create:**
- `infra/nginx/nginx.conf` â€” reverse proxy (api.neuroweave.devâ†’:8000, neuroweave.devâ†’:3000), SSL placeholder, rate limiting 100/min, gzip, security headers
- `.github/workflows/ci.yml` â€” on push/PR: ruff check, pytest, next.js build
- `.github/workflows/deploy.yml` â€” on push to main: docker build + push to registry, SSH deploy
**Verify:** CI workflow syntax is valid (act or manual check), nginx config passes `nginx -t`

---

## Task 15 â€” Tests: Pipeline Unit Tests
**Domain:** Testing
**Depends on:** Task 4 (pipeline complete)
**Files to create:**
- `tests/conftest.py` â€” shared fixtures: mock LLM responses, sample messages, test DB session
- `tests/pipeline/test_disentanglement.py` â€” test clustering: 2 topics get separated, replies stay together, temporal window respected
- `tests/pipeline/test_router.py` â€” mock Claude response â†’ verify NOISE/TECHNICAL classification + edge cases
- `tests/pipeline/test_evaluator.py` â€” mock Claude response â†’ verify JSON parsing, resolved detection, cyclic routing
- `tests/pipeline/test_compiler.py` â€” mock structured output â†’ verify Pydantic validation, error handling
- `tests/pipeline/test_quality_gate.py` â€” test scoring: high/low quality articles score correctly, retry logic works
- `tests/pipeline/test_graph_integration.py` â€” full graph with all mocked LLM nodes â†’ verify end-to-end state flow
**Verify:** `pytest tests/pipeline/ -v` all pass

---

## Task 16 â€” Tests: API Integration Tests
**Domain:** Testing
**Depends on:** Task 7 (all API routers)
**Files to create:**
- `tests/api/test_articles.py` â€” CRUD operations, pagination, filtering
- `tests/api/test_search.py` â€” search endpoint, hybrid ranking
- `tests/api/test_consent.py` â€” create, check, revoke consent
- `tests/api/test_datasets.py` â€” export trigger, list, download
- `tests/api/test_auth.py` â€” OAuth2 flow, JWT validation
**Verify:** `pytest tests/api/ -v` all pass (uses test DB)

---

## Task 17 â€” Tests: Bot + E2E
**Domain:** Testing
**Depends on:** Task 10 (bot commands)
**Files to create:**
- `tests/bot/test_listener.py` â€” message capture, Redis XADD, author hashing
- `tests/bot/test_consent_cmd.py` â€” /privacy command response
- `tests/bot/test_search_cmd.py` â€” /nw-ask command response
**Verify:** `pytest tests/bot/ -v` all pass

---

# PHASE 2 â€” BUGFIXES & MISSING PIECES

These tasks fix critical bugs, add missing services, and make the project actually runnable end-to-end.

## Status tracking

```
[âœ…] Task 18 â€” BUGFIX: Embedding dimension mismatch (1536 â†’ 384)
[âœ…] Task 19 â€” Infrastructure: Start Redis + MongoDB in dev compose
[âœ…] Task 20 â€” Bot: Load monitored channels from DB on startup
[âœ…] Task 21 â€” Seed data script + Makefile
[âœ…] Task 22 â€” PII Anonymizer service (anonymizer.py)
[âœ…] Task 23 â€” Consent enforcement in pipeline
[âœ…] Task 24 â€” Stripe webhook DB updates (remove TODOs)
[âœ…] Task 25 â€” Frontend: Login page + auth state
[âœ…] Task 26 â€” Frontend: Admin dashboard (channels, stats)
[âœ…] Task 27 â€” Frontend: Code syntax highlighting (shiki)
```

---

## Task 18 â€” BUGFIX: Embedding dimension mismatch
**Domain:** Database / Pipeline
**Depends on:** nothing
**Priority:** CRITICAL â€” blocks article storage at runtime
**Problem:** `articles.embedding` column is `vector(1536)` but `api/services/embeddings.py` uses `all-MiniLM-L6-v2` which outputs 384-dim vectors. pgvector will throw a dimension mismatch error when `store_article` tries to save.
**Fix:**
- Change `api/models/article.py`: `embedding = mapped_column(Vector(384))` (was 1536)
- Generate new Alembic migration: `alembic revision --autogenerate -m "fix article embedding dim 384"`
- Apply: `alembic upgrade head`
- Verify: `\d articles` shows `vector(384)`

---

## Task 19 â€” Infrastructure: Redis + MongoDB in dev compose
**Domain:** DevOps
**Depends on:** nothing
**Priority:** CRITICAL â€” Celery and LangGraph checkpoints need these
**Problem:** `docker-compose.yml` defines Redis + MongoDB but only Postgres was started. Also no app services for dev.
**Fix:**
- Ensure `docker compose up -d` starts all 3 (postgres, redis, mongodb)
- Add dev runner script or Makefile with commands to start API, Celery, Bot, Web
- Create `Makefile` with targets: `dev-infra`, `dev-api`, `dev-worker`, `dev-bot`, `dev-web`, `dev-all`
**Verify:** `docker compose up -d` â†’ postgres:5432, redis:6379, mongodb:27017 all healthy

---

## Task 20 â€” Bot: Load monitored channels from DB
**Domain:** Discord Bot
**Depends on:** Task 19 (needs Redis running)
**Priority:** CRITICAL â€” without this, bot listens to nothing
**Problem:** `MessageListener._monitored_channels` is empty set, never populated. `set_monitored_channels()` exists but is never called.
**Fix:**
- In `bot/main.py` `on_ready`: fetch monitored channels from API (`GET /api/servers` â†’ for each server, get channels where `is_monitored=True`)
- Call `listener_cog.set_monitored_channels(channel_ids)`
- Add periodic refresh (every 5 min) in case channels are updated via API
- Add API endpoint or use existing `GET /api/servers/{id}/stats` data
**Verify:** bot starts â†’ logs "monitored_channels_updated count=N" â†’ messages in those channels get published to Redis

---

## Task 21 â€” Seed data + Makefile
**Domain:** DevOps / Database
**Depends on:** Task 18 (needs correct embedding dim), Task 19 (needs all services)
**Priority:** HIGH â€” needed to test the full stack
**Fix:**
- Create `scripts/seed.py`: inserts 1 server, 2 channels, 5 sample messages, 2 threads, 1 article with embedding
- Create `Makefile`:
  ```
  dev-infra:   docker compose up -d
  dev-migrate: .venv/bin/alembic upgrade head
  dev-seed:    .venv/bin/python scripts/seed.py
  dev-api:     .venv/bin/uvicorn api.main:app --reload
  dev-worker:  .venv/bin/celery -A api.celery_app worker -l info -Q extraction,export
  dev-bot:     .venv/bin/python -m bot.main
  dev-web:     cd web && npm run dev
  dev-setup:   dev-infra dev-migrate dev-seed
  ```
**Verify:** `make dev-setup && make dev-api` â†’ `curl localhost:8000/api/servers` returns seeded server

---

## Task 22 â€” PII Anonymizer
**Domain:** Pipeline / Privacy
**Depends on:** Task 21 (needs working pipeline)
**Priority:** HIGH â€” GDPR requirement, messages contain raw usernames/emails
**Problem:** `api/services/anonymizer.py` does not exist. Message content passes through pipeline with real names, emails, IPs intact.
**Fix:**
- Create `api/services/anonymizer.py`:
  - Regex pre-filters: email, IP, phone, URLs with usernames
  - Pattern: `@username` mentions in text â†’ replace with `@[user_HASH]`
  - Pattern: file paths containing usernames `/Users/john/...` â†’ `/Users/[REDACTED]/...`
  - For MVP: regex-only (no Llama 3.2 yet). Add Llama integration later.
- Integrate into `bot/stream_producer.py` or `api/tasks/process_messages.py`: anonymize content before pipeline
- Add tests in `tests/pipeline/test_anonymizer.py`
**Verify:** message "Hey john@gmail.com check 192.168.1.1" â†’ "Hey [EMAIL] check [IP]"

---

## Task 23 â€” Consent enforcement in pipeline
**Domain:** Pipeline / Privacy
**Depends on:** Task 22 (anonymizer should run first)
**Priority:** HIGH â€” GDPR compliance
**Problem:** `process_messages.py` processes all messages regardless of user consent status. The consent API exists but is never checked.
**Fix:**
- In `api/tasks/process_messages.py` before invoking pipeline:
  - Query `consent_records` for each unique `author_hash` in the batch
  - Filter out messages from users who have NOT consented (or revoked)
  - Only pass consented messages to the pipeline
- Add `api/services/consent_checker.py` with `filter_consented_messages(messages, channel_id) â†’ filtered_messages`
**Verify:** user without consent â†’ their messages are excluded from pipeline input

---

## Task 24 â€” Stripe webhook DB updates
**Domain:** Backend / Payments
**Depends on:** nothing
**Priority:** MEDIUM â€” payments work but plans don't update
**Problem:** `api/routers/webhooks.py` has 3 TODO stubs â€” Stripe events are received and verified but Server.plan is never updated.
**Fix:**
- `checkout.session.completed` â†’ find server by `metadata.server_id`, update `plan` to PRO
- `customer.subscription.updated` â†’ update plan tier based on `price_id`
- `customer.subscription.deleted` â†’ downgrade server to FREE
- Add `stripe_customer_id` field to Server model (new migration)
**Verify:** simulate webhook â†’ server.plan changes in DB

---

## Task 25 â€” Frontend: Login page + auth state
**Domain:** Frontend
**Depends on:** nothing
**Priority:** MEDIUM â€” admin features need auth
**Fix:**
- `web/app/login/page.tsx` â€” "Login with Discord" button â†’ redirects to `/api/auth/discord`
- `web/lib/auth.ts` â€” store JWT in localStorage, provide `useAuth()` hook
- `web/components/Navbar.tsx` â€” show login/logout button based on auth state
- `web/app/auth/callback/page.tsx` â€” handle OAuth callback, store token
**Verify:** click login â†’ Discord OAuth â†’ redirected back â†’ Navbar shows username

---

## Task 26 â€” Frontend: Admin dashboard
**Domain:** Frontend
**Depends on:** Task 25 (needs auth)
**Priority:** MEDIUM
**Fix:**
- `web/app/dashboard/page.tsx` â€” server selector, channel toggle, stats overview
- `web/app/dashboard/[serverId]/page.tsx` â€” server management: toggle channels, view articles, moderate
- Uses `GET /api/servers/{id}/stats`, `POST /api/servers/{id}/channels`, `PATCH /api/articles/{id}/moderate`
**Verify:** logged-in admin can toggle channels on/off, see stats

---

## Task 27 â€” Frontend: Code syntax highlighting
**Domain:** Frontend
**Depends on:** nothing
**Priority:** LOW
**Problem:** `CodeBlock.tsx` renders plain `<pre><code>`, no highlighting.
**Fix:**
- Install `shiki` in web/
- Update `CodeBlock.tsx` to use shiki with dark theme
- Auto-detect language from article.language field
**Verify:** code blocks show colored syntax for JS/Python/Rust

---

# PHASE 3 â€” GITHUB DISCUSSIONS + MULTI-SOURCE PIPELINE

Expand beyond Discord: add GitHub Discussions as a data source, broaden pipeline to handle any content type (not just code/technical).

## Principles
- Articles don't require code to be created
- Content is NOT limited to "technical" â€” Q&A, guides, discussions are all valuable
- GitHub Discussions are already threaded â†’ skip disentanglement
- Public GitHub data â†’ skip consent check
- Backward compatible with Discord

## Status tracking

```
[âœ…] Task 28 â€” DB Schema: Source-Agnostic Generalization
[âœ…] Task 29 â€” Fix Hard Couplings (generate_article, consent_checker, export)
[âœ…] Task 30 â€” Pipeline: Broaden Router, Evaluator, Compiler for all content types
[âœ…] Task 31 â€” GitHub Discussions Fetcher (GraphQL API + Celery periodic)
[âœ…] Task 32 â€” API: GitHub Repo Management endpoints
[âœ…] Task 33 â€” Frontend: Source Badges + GitHub UI
[âœ…] Task 34 â€” Seed Data + CLI for GitHub sources
[âœ…] Task 35 â€” Tests: GitHub + expanded pipeline
```

---

## Task 28 â€” DB Schema: Source-Agnostic Generalization
**Domain:** Database
**Depends on:** nothing
**Priority:** CRITICAL
**Files to modify:**
- `api/models/server.py` â€” add `source_type` enum (discord/github/discourse), `external_id`, `source_url`, `source_metadata` JSONB
- `api/models/channel.py` â€” add `external_id`, make `discord_id` nullable
- `api/models/message.py` â€” add `external_id`, make `discord_message_id` nullable
- `api/models/article.py` â€” add `article_type` enum (troubleshooting/question_answer/guide/discussion_summary), `source_type`, `source_url`
- New Alembic migration: add columns â†’ backfill â†’ NOT NULL â†’ unique constraints
**Verify:** `alembic upgrade head` succeeds, existing Discord data has `source_type='discord'` and `external_id` populated

---

## Task 29 â€” Fix Hard Couplings
**Domain:** Backend
**Depends on:** Task 28
**Files to modify:**
- `api/tasks/generate_article.py` â€” `Channel.discord_id` â†’ `Channel.external_id`, add `source_type` param
- `api/tasks/process_messages.py` â€” pass `source_type`, skip consent for github
- `api/services/consent_checker.py` â€” `servers.discord_id` â†’ `servers.external_id`, skip for github
- `api/tasks/export_dataset.py` â€” dynamic source string
**Verify:** `store_article(channel_id="DIC_kwDO123", source_type="github")` â†’ article stored

---

## Task 30 â€” Pipeline: Broaden Router, Evaluator, Compiler
**Domain:** Pipeline
**Depends on:** Task 28
**Files to modify:**
- `api/services/extraction/state.py` â€” add `source_type`, `article_type`, `skip_disentangle` to AgentState
- `api/services/extraction/nodes/router.py` â€” 5 categories: NOISE/TROUBLESHOOTING/QUESTION_ANSWER/GUIDE/DISCUSSION_SUMMARY
- `api/services/extraction/nodes/evaluator.py` â€” Q&A passes without code, GUIDE/DISCUSSION always pass
- `api/services/extraction/nodes/compiler.py` â€” flexible ExtractedKnowledge (language optional, article_type, source_url)
- `api/services/extraction/nodes/quality_gate.py` â€” type-aware scoring (non-code articles not penalized)
- `api/services/extraction/graph.py` â€” skip_disentangle support
**Verify:** Q&A without code â†’ quality â‰¥ 0.7; existing TROUBLESHOOTING unchanged

---

## Task 31 â€” GitHub Discussions Fetcher
**Domain:** Data Ingestion
**Depends on:** Task 28, Task 29
**Files to create:**
- `api/services/github_fetcher.py` â€” GitHubDiscussionsFetcher class (GraphQL API, fetch + convert to messages)
- `api/tasks/fetch_github_discussions.py` â€” Celery periodic task (15 min), fetch new discussions per github server
**Files to modify:**
- `api/config.py` â€” add GITHUB_TOKEN
- `api/celery_app.py` â€” register periodic task
**Verify:** Fetch discussions from `vercel/next.js`, pipeline produces articles with `source_type="github"`

---

## Task 32 â€” API: GitHub Repo Management
**Domain:** Backend API
**Depends on:** Task 31
**Files to create:**
- `api/schemas/github.py` â€” GitHubRepoCreate, GitHubRepoResponse
- `api/routers/github.py` â€” POST /api/github/repos, GET /api/github/repos, POST sync, DELETE
**Files to modify:**
- `api/routers/search.py` â€” add `source` filter param
- `api/schemas/article.py` â€” add article_type, source_type, source_url
- `api/schemas/server.py` â€” add source_type, external_id, source_url
- `api/main.py` â€” include github_router
**Verify:** POST repo â†’ POST sync â†’ GET search?source=github â†’ articles appear

---

## Task 33 â€” Frontend: Source Badges + GitHub UI
**Domain:** Frontend
**Depends on:** Task 32
**Files to create:**
- `web/components/SourceBadge.tsx`, `web/components/ArticleTypeBadge.tsx`
- `web/app/dashboard/github/page.tsx`
**Files to modify:**
- `web/components/ArticleCard.tsx` â€” source + type badges
- `web/app/page.tsx` â€” mixed server/repo grid
- `web/app/articles/[id]/page.tsx` â€” "View on GitHub" link
- `web/app/search/page.tsx` â€” source filter dropdown
**Verify:** Homepage shows Discord + GitHub, articles have badges, search filters by source

---

## Task 34 â€” Seed Data + CLI for GitHub
**Domain:** DevOps
**Depends on:** Task 32
**Files to create:**
- `scripts/seed_github.py` â€” sample GitHub server + Q&A articles
- `scripts/fetch_github.py` â€” CLI: `python scripts/fetch_github.py vercel/next.js --limit 10`
**Verify:** `make dev-seed-github` â†’ search returns GitHub articles

---

## Task 35 â€” Tests: GitHub + Expanded Pipeline
**Domain:** Testing
**Depends on:** Tasks 28-32
**Files to create:**
- `tests/pipeline/test_router_expanded.py` â€” 5 categories
- `tests/pipeline/test_evaluator_expanded.py` â€” Q&A Ğ±ĞµĞ· ĞºĞ¾Ğ´Ğ° passes
- `tests/pipeline/test_compiler_expanded.py` â€” flexible article types
- `tests/pipeline/test_quality_gate_expanded.py` â€” non-code scores â‰¥ 0.7
- `tests/pipeline/test_github_fetcher.py` â€” GraphQL mock
- `tests/api/test_github.py` â€” CRUD endpoints
**Verify:** all new tests pass, existing 115 tests still pass

---

# PHASE 4 â€” UX & PRODUCT QUALITY

Transform from demo to real product. 51+ articles exist with 95% avg quality â€” content is good, but discovery, search, and engagement are weak.

## Key Problems
1. No content discovery â€” homepage shows server list, no articles preview
2. Search has no filters, no sort, no suggestions
3. Zero user feedback loop â€” quality scores are ML metrics, not user-validated
4. Article pages don't retain users â€” no related articles, no "what next"
5. Raw ML scores (92% confidence) confuse regular users

## Status tracking

```
[ ] Task 36 â€” Homepage: Stats + Recent Articles + Trending Tags
[ ] Task 37 â€” Search: Filter Chips + Sort + Better Results
[ ] Task 38 â€” Article Page: Related Articles + Reading Time
[ ] Task 39 â€” User Feedback: "Was this helpful?" + View Counter
[ ] Task 40 â€” Browse Page: by Language, Type, Tag
[ ] Task 41 â€” Better Display: Badges, Breadcrumbs, Polish
[ ] Task 42 â€” Cmd+K Instant Search (modal overlay)
[ ] Task 43 â€” SEO: Meta Tags, Open Graph, Sitemap
```

---

## Task 36 â€” Homepage: Stats + Recent Articles + Trending Tags
**Domain:** Frontend + API
**Depends on:** nothing
**Priority:** HIGH â€” first impression for new users
**Changes:**
- Hero section: show "N articles across M languages" dynamically
- Add "Recent Articles" section â€” last 6 articles as ArticleCards
- Add "Trending Tags" section â€” top 10 tags with article counts
- Rename "Servers" â†’ "Knowledge Sources"
- Remove "plan: FREE" from server cards (admin noise)
**API:** `GET /api/articles/recent?limit=6`, `GET /api/stats/overview`
**Verify:** Homepage shows article count, recent articles, trending tags

---

## Task 37 â€” Search: Filter Chips + Sort + Better Results
**Domain:** Frontend + API
**Depends on:** nothing
**Priority:** HIGH â€” search is the #1 user action
**Changes:**
- Horizontal filter chips: Language (Python, JS, General...), Type (Bug Fix, Q&A, Guide), Source (Discord, GitHub)
- Sort options: Most Relevant / Newest / Highest Quality
- Replace raw score "0.847" â†’ "Best match" / "Good match" or hide
- Empty state: "Try: React hooks, Python async, Docker networking"
- Fix per-server SearchBar to scope to current server
**API:** add `sort` param to `GET /api/search`, add `GET /api/search/facets?q=` for filter counts
**Verify:** Search page shows filter chips, sort works, per-server search scoped

---

## Task 38 â€” Article Page: Related Articles + Reading Time
**Domain:** Frontend + API
**Depends on:** nothing
**Priority:** HIGH â€” keeps users on site
**Changes:**
- "Related Articles" section at bottom: 4 similar articles via pgvector embedding distance
- Reading time estimate: "3 min read" based on word count (~200 wpm)
- Type-aware section labels: Q&A â†’ "Question / Context / Answer", Guide â†’ "Topic / Prerequisites / Guide"
- Better "View original discussion" as a button, not text link
**API:** `GET /api/articles/{id}/related?limit=4`
**Verify:** Article page shows related articles, reading time, type-aware labels

---

## Task 39 â€” User Feedback: "Was this helpful?" + View Counter
**Domain:** Full-stack (DB + API + Frontend)
**Depends on:** nothing
**Priority:** HIGH â€” closes the quality feedback loop
**Changes:**
- Add to Article model: `views` (int, default 0), `helpful_yes` (int, default 0), `helpful_no` (int, default 0)
- New Alembic migration
- `POST /api/articles/{id}/view` â€” increment views (no auth, fire-and-forget)
- `POST /api/articles/{id}/feedback` â€” `{helpful: true/false}` (no auth)
- Article page: fire view on load, show "Was this helpful? ğŸ‘ Yes (23) ğŸ‘ No (2)" at bottom
- ArticleCard: show "ğŸ‘ 142" view count
- Replace raw quality_score with "Verified âœ“" badge (if quality â‰¥ 0.8)
**Verify:** Views increment on page load, feedback buttons work, counts display

---

## Task 40 â€” Browse Page: by Language, Type, Tag
**Domain:** Frontend + API
**Depends on:** nothing
**Priority:** MEDIUM â€” discovery without search
**Changes:**
- New page `/browse` with three sections:
  - By Language: Python (12), JavaScript (8), General (28) â€” clickable cards
  - By Type: Bug Fixes (12), Q&A (19), Guides (9), Discussions (11)
  - By Tag: top 20 tags as chips with counts
- Each click â†’ `/search?language=X` or `/search?type=Y`
- Add "Browse" link to Navbar
**API:** `GET /api/stats/browse` â€” aggregated counts
**Verify:** Browse page shows language/type/tag grids with real counts

---

## Task 41 â€” Better Display: Badges, Breadcrumbs, Polish
**Domain:** Frontend
**Depends on:** Task 39 (needs view count)
**Changes:**
- Breadcrumb component: Home > JavaScript > Bug Fixes > Article Title
- Replace "confidence: 92%" â†’ "AI Verified âœ“" green badge (if > 0.8)
- Replace "quality: 99%" â†’ hide or show as star rating
- ArticleCard: add reading time "3 min" + view count "ğŸ‘ 142"
- Article page: add breadcrumbs at top
**Verify:** Articles show breadcrumbs, badges instead of raw scores

---

## Task 42 â€” Cmd+K Instant Search (modal overlay)
**Domain:** Frontend
**Depends on:** nothing
**Priority:** MEDIUM â€” power user feature
**Changes:**
- Cmd+K / Ctrl+K opens full-screen modal search (like Vercel, Linear, Algolia DocSearch)
- Debounced API call as user types (300ms)
- Top 5 results shown inline: title + type badge + language
- Arrow keys navigate, Enter opens article, Esc closes
- Recent searches in localStorage
**Verify:** Cmd+K opens overlay, typing shows instant results, keyboard nav works

---

## Task 43 â€” SEO: Meta Tags, Open Graph, Sitemap
**Domain:** Frontend
**Depends on:** nothing
**Priority:** MEDIUM â€” organic growth
**Changes:**
- Dynamic `<title>` per article: "{summary} | NeuroWeave"
- Dynamic `<meta description>`: first 160 chars of symptom
- Open Graph tags: og:title, og:description, og:type=article
- `/sitemap.xml` API endpoint listing all visible articles
- `robots.txt` allowing crawling
**Verify:** View page source shows correct meta tags, /sitemap.xml returns XML
